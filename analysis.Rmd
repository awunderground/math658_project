---
output:
  pdf_document
fontsize: 12pt
editor_options:
  chunk_output_type: console
---

```{r rmarkdown-setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(size = "footnotesize")
knitr::opts_chunk$set(dev = "cairo_pdf")

options(scipen = 999)

def.chunk.hook <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size, "\n\n", x, "\n\n \\normalsize"), x)
})
```

```{r r-setup}
library(tidyverse)
library(knitr)
```

## Sampling Frame

### Download the data

```{r download-data}
# file path to csv with addresses
aru_file_path <- 
  "https://opendata.arcgis.com/datasets/c3c0ae91dca54c5d9ce56962fa0dd645_68.csv"

ap_file_path <- 
  "https://opendata.arcgis.com/datasets/aa514416aaf74fdc94748f1e56e7cc8a_0.csv"

# create a directory for downloading the data
if (!dir.exists("data/")) {
  dir.create("data")
}

# if the data doesn't already exist, download the data
if (!file.exists("data/aru.csv")) {
  download.file(aru_file_path, "data/aru.csv")
}

if (!file.exists("data/ap.csv")) {
  download.file(ap_file_path, "data/ap.csv")
}
```

### Address Residential Units

The first dataset is [Address Residential Units](http://opendata.dc.gov/datasets/address-residential-units)

The dataset does not contain a variable for quadrant, so we extract quadrant from the full address.

```{r load-aru}
aru <- read_csv("data/aru.csv") %>%
  rename_all(tolower) %>%
  select(unit_id, address_id, fulladdress, status, unitnum, unittype)

# extract quadrant
aru <- aru %>%
  mutate(quadrant = str_sub(fulladdress, start = -2, end = -1))
```

Address Residential Units contains residential units with status set to "RETIRED". We drop these cases as well. 

```{r}
count(aru, status) %>%
  kable()
```

```{r}
aru <- aru %>%
  filter(status != "RETIRE")
```

### Adress Points

```{r load-ap}
# load the data and convert the variable names to lower case
ap <- read_csv("data/ap.csv", guess_max = 10000) %>%
  rename_all(tolower) %>%
  select(address_id, status, type_, entrancetype, quadrant, fulladdress, 
         objectid_1, assessment_nbhd, cfsa_name, census_tract, vote_prcnct, 
         ward, zipcode, anc, census_block, census_blockgroup, latitude, 
         longitude, active_res_unit_count, res_type, active_res_occupancy_count) 
```

Address Points contains residential units, non-residential units, and mixed-use units. Residential units and mixed-use units contain residences that belong to our sampling frame. We drop non-residential units. 

```{r}
count(ap, res_type) %>%
  kable()
```

```{r}
ap <- ap %>%
  filter(res_type != "NON RESIDENTIAL")
```

Address points contains residential units with status set to "RETIRED". We drop these cases as well. 

```{r}
count(ap, status) %>%
  kable()
```

```{r}
ap <- ap %>%
  filter(status != "RETIRE")
```

After the above filtering, there are 98 observations from Address Points and 3,706 observations in Address Residential Units that have missing addresses. We investigated joining the two datasets on `address_id` to fill in the address but all records missing an address in one dataset were missing an address in the other dataset. 

We dropped the missing values which represented about 1.5 percent of observations in Address Residential Units and 0.07 percent of observations in Address Points. 

```{r}
ap <- ap %>%
  filter(!is.na(fulladdress))

aru <- aru %>%
  filter(!is.na(fulladdress))
```

```{r include = FALSE}
missing_aru <- filter(aru, is.na(fulladdress))

missing_aru <- left_join(missing_aru, ap, by = "address_id")

# check the complement to see if any rows didn't match
if (nrow(anti_join(missing_aru, ap, by = "address_id")) > 0) "ERROR"


count(missing_aru, fulladdress.y)

missing_ap <- filter(ap, is.na(fulladdress))

missing_ap <- left_join(missing_ap, aru, by = "address_id")

if (nrow(anti_join(missing_ap, aru, by = "address_id")) > 0) "ERROR"

count(missing_ap, fulladdress.y)

rm(missing_ap, missing_aru)
```

### Merge variables

Address Points has interesting variables not present in Address Residential Units. So we merge the Address Points dataset with the Address Residential Units dataset. The join works for all but 572 cases, most of which are in a new building at the Wharf. 

```{r}
aru_expanded <- aru %>%
  select(-status) %>%
  left_join(ap, by = c("fulladdress", "address_id")) %>%
  select(quadrant = quadrant.x, everything(), -quadrant.y)

anti_join(aru, ap, by = c("fulladdress", "address_id"))
```

### Combination

Next, we need to drop addresses in the Address Points dataset that exist in the Address Residential Units dataset so we don't overcount addresses in multi-dwelling units. 

```{r}
ap <- ap %>%
  filter(!address_id %in% unique(aru_expanded$address_id))
```

Finally, we can combine the two datasets to create a sampling frame that contains approximately every residential address in Washington D.C.


```{r}
sampling_frame <- bind_rows(ap, aru_expanded) 

#summarize_all(addresses, list(~sum(is.na(.))))

write_csv(sampling_frame, "sampling_frame.csv")
```









```{r}
filter(aru, str_detect(fulladdress, "1930 NEW HAMPSHIRE"))

filter(ap, str_detect(fulladdress, "1930 NEW HAMPSHIRE"))

```

## Pilot survey

```{r write-pilot-survey}
set.seed(20190714)

pilot_sample <- sampling_frame %>%
  group_by(quadrant) %>%
  sample_n(25)

write_csv(pilot_sample, "data/pilot_sample.csv")

rm(pilot_sample)
```

## Picking stratum sizes

### Sample mean

We begin with a derivation of the formula for calculating priority values for $\bar{y}$.

Decomposition of $V(\bar{y}_h)$ for stratum $h$:

< will add later>

Decomposition of $V(\bar{y}_{str})$

< will add later>

Many of the terms int he calculation of prioirty values will be vary large, so we can express them as 

$\frac{N_1S_1}{\sqrt{1 \cdot 2}}$, $\frac{N_2S_2}{\sqrt{1 \cdot 2}}$, ..., $\frac{N_HS_H}{\sqrt{1 \cdot 2}}$ (12.36)

\hfill

For a desired bound $V_0$ on the sampling variance $V(\bar{y}_{str})$, we may find an optimal allocation using the followng algorithm:

1) Assign, for each stratum, 1 unit to be selected for the sample. 

2) Fill in the following table and number these values starting from 1, in decreasing order. 

| | | | |
|---|---|---|---|
| $\frac{N_1S_1}{\sqrt{1 \cdot 2}}$ | $\frac{N_1S_1}{\sqrt{2 \cdot 3}}$ | $\frac{N_1S_1}{\sqrt{3 \cdot 4}}$ | ... |
| $\frac{N_2S_2}{\sqrt{1 \cdot 2}}$ | $\frac{N_2S_2}{\sqrt{2 \cdot 3}}$ | $\frac{N_2S_2}{\sqrt{3 \cdot 4}}$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\frac{N_HS_H}{\sqrt{1 \cdot 2}}$ | $\frac{N_HS_H}{\sqrt{2 \cdot 3}}$ | $\frac{N_HS_H}{\sqrt{3 \cdot 4}}$ | ... |

3) Since the initial allocation is $(n_{11}, n_{21}, ..., n_{H1}) = (1, 1, ..., 1)$, compute $V(\bar{y}_{str}|n_{11} = 1, n_{21} = 1, ..., n_{H1} = 1) = \frac{1}{N^2} \sum_{h = 1}^H ((N^2_h - N_h) S^2_h)$

4) Pick value (1) from the table and increase the associated stratum's sample size by 1, o that the updated allocation is $(n_{12}, n_{22}, ..., n_{H2})$, where exactly one of the $n_{h2}$'s is equal to 2 and the rest are equal to 1. Then, compute $V(\bar{y}_{str}|n_{12}, ..., n_{H2} = V(\bar{y}_{str}|n_{11}, ..., n_{H1}) - \frac{1}{N^2}$ where "(1)" represents the largest value from the table. If $V(\bar{y}_{str}|N_{12}, ..., n_{H2} \leq V_0$, then stop with $n_1 = n_{12}, ..., N_H = N_{H2}$. Otherwise, go to step 5. 

5) Pick value (2) from the table and increase the associated stratum's sample size by 1, so that the updated allocation is $(n_{13}, ..., n_{H3})$. Then compute $V(\bar{y}_{str}|n_{13}, ..., n_{H3}) = V(\bar{y}_{str}|n_{12}, ..., n_{H2} - \frac{(2)}{N^2}$, where "(2)" represents the second value from the table. If $V(\bar{y}_{str}|n_{13}, ..., N_H = n_{H3}$. Otherwise, continue until step $j$, where $V(\bar{y}_str|n_{1j}, ..., n_{Hj}) \leq V_0$. The final allocation is $n_{1j}, ..., n_{Hj})$ and $n = n_{1j} + \cdot\cdot\cdot + n_{Hj}$.

To find an optimal allocation for $V(\hat{p}_{str})$, proceed in the same manner as above, but with $V(\hat{p}_{str}|n_{11} = 1) = (\frac{1}{N^2}\sum_{h = 1}^H(N^2_hp_h(1-p_h))$. Instead of using a pilot survey, we use $\hat{p} = 0.5$ to get the theoretical maximum for a proportion. 

\hfill

```{r}
# load the completed pilot survey and clean the values
pilot_sample <- read_csv("data/pilot_sample_completed.csv") %>%
  mutate(land_value = ifelse(!is.na(rf_land_value), 
                             rf_land_value, 
                             land_value),
         improvement_value = ifelse(!is.na(rf_improvement_value), 
                                    rf_improvement_value, 
                                    improvement_value)) %>%
  mutate(property_value = land_value + improvement_value) %>%
  mutate(property_value = ifelse(unittype == "RENTAL" & 
                                   active_res_occupancy_count > 4 &
                                   property_value > 2000000,
                                 property_value / active_res_occupancy_count,
                                 property_value
                                 ))
```

```{r}
# find Nh and s2 for each strata
# (1) and (2)
s_squared_h <- pilot_sample %>%
  group_by(stratum = quadrant) %>%
  summarize(s_squared_h = var(property_value, na.rm = TRUE), 
            missing_prop = mean(is.na(property_value)))

Nh <- sampling_frame %>%
  count(stratum = quadrant) %>%
  rename(Nh = n)

strata <- left_join(s_squared_h, Nh, by = "stratum") %>%
  # adjust N because of missingness  
  mutate(Nh = Nh * (1 - missing_prop)) %>%
  mutate(N = sum(Nh))

rm(s_squared_h, Nh)

kable(strata)
```

Step 3: $\hat{V}(\bar{y}|1, 1, 1, 1) = \sum_{h = 1}^H (\frac{N_h}{N})^2 \frac{N_h - n_h}{N_H} \frac{s^2_h}{n_h} = \sum_{h = 1}^H (\frac{N_h}{N})^2 \frac{N_h - 1}{N_H} \frac{s^2_h}{1}$ 

(Wright 12.5)

```{r mean-initial-allocation}
# Let the initial allocation be (n_11, n_21, n_31, n_41) = (1, 1, 1, 1) 
# (3) and (6)
starting_variance <- strata %>%
  mutate(strata_variance = (Nh / N) ^ 2 * ((Nh - 1) / Nh) * (s_squared_h / 1)) 

kable(starting_variance)

starting_variance <- starting_variance %>%
  summarize(V = sum(strata_variance)) %>%
  pull()

starting_variance
```


Step 3: 

Prioirty value = $\frac{N_1^2 \cdot s_1^2}{N_1^2 \cdot n_h(n_h - 1)}$

```{r nean-prioirty-values}
# create a table of priority values
# (4) and (5)
n_strata <- 
  tibble(n = c(1:500, 1:500, 1:500, 1:500),
       stratum = c(rep("NE", 500), rep("NW", 500), rep("SE", 500), rep("SW", 500))) %>%
  left_join(strata, by = "stratum")

# step 2
priority_values <- n_strata %>%
  group_by(stratum) %>%
  mutate(priority_value = (Nh ^ 2 * s_squared_h) / (n * lag(n) * N ^ 2)) %>%
  ungroup() %>%
  arrange(desc(priority_value))

kable(head(select(priority_values, -missing_prop), n = 10))
```

Step 4:

```{r}
# (7)
priority_values <- priority_values %>%
  mutate(agg_priority_value = cumsum(priority_value)) %>%
  mutate(marginal_variance = starting_variance - agg_priority_value) %>%
  mutate(marginal_sd = sqrt(marginal_variance))

kable(head(select(priority_values, -missing_prop), n = 50), digits = 0)
```

### Proportion

Determining exact optimal allocation for $\hat{p}$ is different than $\bar{y}$. We begin with a derivation of the formula for calculating priority values for $\hat{p}$.

\hfill

For $p_h$, $S^2_h = [\frac{N_h}{N_h - 1}] p_h(1 - p_h)$.

$V(\hat{p}_{str}) = \frac{1}{N^2}(\sum_{h = 1}^H N_h(N_h - 1)S_h^2 - \frac{N_1^2S_1^2}{1 \cdot 2} - \cdot\cdot\cdot - \frac{N_H^2S_H^2}{(n_H - 1)n_H})$

$V(\hat{p}_{str}) = \sum_{h = 1}^H \frac{N_h(N_h - 1) S_h^2}{N^2} - \frac{N_1^2S_1^2}{N^2(1 \cdot 2)} - \cdot\cdot\cdot \frac{N_H^2 S_H^2}{N^2 (n_H - 1)(n_H)}$

$V(\hat{p}_{str}) = \sum_{h = 1}^H \frac{N_h(N_h - 1) S_h^2}{N^2} - \frac{N_1^3 p_1(1 - p_1)}{N^2(1 \cdot 2)} - \cdot\cdot\cdot \frac{N_H^3 p_H(1 - p_H)}{N^2 (n_H - 1)(n_H)}$

\hfill

For a desired bound on $V_0$ on the sampling variance $V(\hat{p}_{str})$, we may find an optimal allocation using the following algorithm:

1) Assign, for each stratum, 1 unit to be selected for the sample. 

2) Fill in the following table and number these values starting from 1, in decreasing order. We assume $p_h = 0.5$ because that is where the variance reaches its global maximum.

| | | | |
|---|---|---|---|
| $\frac{\frac{1}{4}N_1^3}{N^2 \cdot 1 \cdot 2}$ | $\frac{\frac{1}{4}N_1^3}{N^2 \cdot 2 \cdot 3}$ | $\frac{\frac{1}{4}N_1^3}{N^2 \cdot 3 \cdot 4}$ | ... |
| $\frac{\frac{1}{4}N_2^3}{N^2 \cdot 1 \cdot 2}$ | $\frac{\frac{1}{4}N_2^3}{N^2 \cdot 2 \cdot 3}$ | $\frac{\frac{1}{4}N_2^3}{N^2 \cdot 3 \cdot 4}$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\frac{\frac{1}{4}N_H^3}{N^2 \cdot 1 \cdot 2}$ | $\frac{\frac{1}{4}N_H^3}{N^2 \cdot 2 \cdot 3}$ | $\frac{\frac{1}{4}N_H^3}{N^2 \cdot 3 \cdot 4}$ | ... |

3) Since the initial allocation is $(n_{11}, n_{21}, ..., n_{H1}) = (1, 1, ..., 1)$, compute $V(\hat{p}_{str}|n_{11} = 1, n_{21} = 1, ..., n_{H1} = 1) = \frac{1}{N^2} \sum_{h = 1}^H ((N^2_h - N_h) S^2_h)$

4) Pick value (1) from the table and increase the associated stratum's sample size by 1, o that the updated allocation is $(n_{12}, n_{22}, ..., n_{H2})$, where exactly one of the $n_{h2}$'s is equal to 2 and the rest are equal to 1. Then, compute $V(\hat{p}_{str}|n_{12}, ..., n_{H2} = V(\hat{p}_{str}|n_{11}, ..., n_{H1}) - \frac{1}{N^2}$ where "(1)" represents the largest value from the table. If $V(\hat{p}_{str}|N_{12}, ..., n_{H2} \leq V_0$, then stop with $n_1 = n_{12}, ..., N_H = N_{H2}$. Otherwise, go to step 5. 

5) Pick value (2) from the table and increase the associated stratum's sample size by 1, so that the updated allocation is $(n_{13}, ..., n_{H3})$. Then compute $V(\hat{p}_{str}|n_{13}, ..., n_{H3}) = V(\hat{p}_{str}|n_{12}, ..., n_{H2} - \frac{(2)}{N^2}$, where "(2)" represents the second value from the table. If $V(\hat{p}_{str}|n_{13}, ..., N_H = n_{H3}$. Otherwise, continue until step $j$, where $V(\hat{p}_{str}|n_{1j}, ..., n_{Hj}) \leq V_0$. The final allocation is $n_{1j}, ..., n_{Hj})$ and $n = n_{1j} + \cdot\cdot\cdot + n_{Hj}$.













