---
output:
  pdf_document
fontsize: 12pt
editor_options:
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r rmarkdown-setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
#knitr::opts_chunk$set(include = FALSE)
knitr::opts_chunk$set(size = "footnotesize")
knitr::opts_chunk$set(dev = "cairo_pdf")

options(scipen = 999)

def.chunk.hook <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size, "\n\n", x, "\n\n \\normalsize"), x)
})
```

# Zoning out: an analysis of zoning and property values in Washington, D.C.

### Brian Bontempo, Derrick Lee, and Aaron R. Williams

\hfill

```{r out.width = "50%", echo = FALSE, fig.align = "center"}
knitr::include_graphics("images/1621T.JPG")
```

\newpage

## Introduction

## 1. Need Statement

Washington D.C. is consistently ranked as one of the most-expensive places to live in America (@Goetz19, @Frohlich19, @Kiersz19). A main driver of affordability challenges in the District is housing. Rents and home purchase prices have grown dramatically during the last two decades and affordability challenges have surged across the community's 68 square miles. At the same, Washington D.C.'s restrictive zoning, height limitation, and excessive historical preservation have constrained housing supply during a period of robust economic and population growth. 

This study aims to understand the cost of housing and the nature of zoning in Washington, D.C. and in its four quadrants. As subsequent sections will demonstrate, there are rich sources of information about property values and zoning rules in Washington, D.C., but it is difficult to extract information about population parameters from those sources. A probability sample of residences is to be taken to estimate the value of residential properties and the proportion of properties with zoning that allows for multi-family housing. 

## 2. Target Population

Our target population is the set of all official residential housing units in the city of Washington, D.C., or at least all residential housing units included in the Master Address Repository. This population explicitly excludes any government or commercial units such as offices, museums, and public spaces. The target population also includes both occupied and unoccupied residential units, as well as individual units contained in multi-family residential buildings. 

## 3. Sampling Frame

- I. Address Residential Units 
  - https://opendata.dc.gov/datasets/c3c0ae91dca54c5d9ce56962fa0dd645_68
- II. Address Points 
  - http://opendata.dc.gov/datasets/address-points

To obtain a list of all residential housing units in Washington, D.C., we sourced two datasets prepared by The District of Columbia Geographical Information System (DC GIS) on June 25th, 2019. The first dataset, Address Residential Units(I), contains all Multifamily residential units and attributes specifically pertaining to units within condominiums and apartments. With a total of 251,180 records, the dataset is extensive as it lists individual units of these Multifamily complexes in DC. The set contains multiple housing units for each street address with units being differentiated by their unit numbers (ie. Apt. 26). 

The second dataset, Address Point(II), is a comprehensive list of all primary addresses within D.C. It includes 147,650 records of Single-Family, Multifamily, and non-residential addresses. Unlike the first dataset, the Address Point dataset has more variables to describe each address, such as ward ID, census block ID, and active residential occupancy counts. However, the dataset does not list the individual units within the Multifamily complexes - but rather just lists street addresses for the complexes. An apartment building that shows up as many addresses in the Address Residential Units dataset only shows up once in this data set. 

To form our Sampling Frame, we merged the two datasets - extracting key data points from each dataset. This required dropping street addresses from the Address Points dataset that appear in Address Residential Units. This avoided double counting. We were able to generate a final list of all residential housing units in Washington, D.C., comprised of 348,094 Single-family and Multi-family units. The R code in the appendix includes all transformations used to create the final dataset. Both source datasets and the final dataset include quadrant for every obsevration, which will prove valuable for stratifcation. 

```{r r-setup}
# load necessary packages
library(tidyverse)
library(knitr)
library(urbnmapr)
library(urbnthemes)
library(survey)

set_urbn_defaults(style = "print")
```

```{r download-data}
# Download the data
# file path to csv with addresses
aru_file_path <- 
  "https://opendata.arcgis.com/datasets/c3c0ae91dca54c5d9ce56962fa0dd645_68.csv"

ap_file_path <- 
  "https://opendata.arcgis.com/datasets/aa514416aaf74fdc94748f1e56e7cc8a_0.csv"

# create a directory for downloading the data
if (!dir.exists("data/")) {
  dir.create("data")
}

# if the data doesn't already exist, download the data
if (!file.exists("data/aru.csv")) {
  download.file(aru_file_path, "data/aru.csv")
}

if (!file.exists("data/ap.csv")) {
  download.file(ap_file_path, "data/ap.csv")
}
```

```{r load-aru}
# Adress Residential Units
# The dataset does not contain a variable for quadrant, so we extract quadrant 
# from the full address.
aru <- read_csv("data/aru.csv") %>%
  rename_all(tolower) %>%
  select(unit_id, address_id, fulladdress, status, unitnum, unittype)

# extract quadrant
aru <- aru %>%
  mutate(quadrant = str_sub(fulladdress, start = -2, end = -1))
```

```{r}
# ARU contains about 7,000 residential units with status set to "RETIRED". 
# We drop these cases. 
aru <- aru %>%
  filter(status != "RETIRE")
```

```{r load-ap}
# Address Points
# load the data and convert the variable names to lower case
ap <- read_csv("data/ap.csv", guess_max = 10000) %>%
  rename_all(tolower) %>%
  select(address_id, status, type_, entrancetype, quadrant, fulladdress, 
         objectid_1, assessment_nbhd, cfsa_name, census_tract, vote_prcnct, 
         ward, zipcode, anc, census_block, census_blockgroup, latitude, 
         longitude, active_res_unit_count, res_type, active_res_occupancy_count) 
```

```{r}
# AP contains residential units, non-residential units, and mixed-use units. 
# Residential units and mixed-use units contain residences that belong to our 
# sampling frame. 
# We drop non-residential units. 
ap <- ap %>%
  filter(res_type != "NON RESIDENTIAL")
```

```{r}
# Address points contains residential units with status set to "RETIRED". 
# We drop these cases as well.
ap <- ap %>%
  filter(status != "RETIRE")
```

```{r}
# After the above filtering, there are 98 observations from Address Points and 
# 3,706 observations in Address Residential Units that have missing addresses. 
# We investigated joining the two datasets on `address_id` to fill in the 
# address but all records missing an address in one dataset were missing an 
# address in the other dataset. We dropped the missing values which represented 
# about 1.5% of observations in Address Residential Units and 0.07% of 
# observations in Address Points.
ap <- ap %>%
  filter(!is.na(fulladdress))

aru <- aru %>%
  filter(!is.na(fulladdress))
```

```{r}
# Address Points has interesting variables not present in ARU. 
# So we merge the Address Points dataset with the Address Residential Units 
# dataset. The join works for all but 572 cases, most of which are in a new 
# building at the Wharf. 
aru_expanded <- aru %>%
  select(-status) %>%
  left_join(ap, by = c("fulladdress", "address_id")) %>%
  select(quadrant = quadrant.x, everything(), -quadrant.y)

rm(aru)
```

```{r}
### Combination

# Next, we need to drop addresses in the Address Points dataset that exist in 
# the ARU dataset so we don't over count addresses in multi-dwelling units.

ap <- ap %>%
  filter(!address_id %in% unique(aru_expanded$address_id))
```

```{r}
# Finally, we can combine the two datasets to create a sampling frame that 
# contains approximately every residential address in Washington D.C.

sampling_frame <- bind_rows(ap, aru_expanded) 

rm(ap, aru_expanded)

write_csv(sampling_frame, "data/sampling_frame.csv")
```

## 4. Primary Parameters

Our primary parameters of interest were the average appraised property value of the D.C. residential units in our sampling frame and the proportion of units that allow for multi-family housing like apartments and condos. Our goal was not only to produce reliable estimates for the entire target population (for which we used a stratified random sample), but also to obtain reliable estimates for each of our strata -- i.e., for the NW, NE, SW, and SE quadrants of D.C. Furthermore, we sought to estimate the difference between the average appraised property of D.C. housing units in areas that allow for multi-family housing and the average appraised value of housing units in areas that do not allow for multi-family housing.  

- Mean appraised property value in Washington, DC
- Mean appraised property value in each quadrant of Washington, DC
- Proportion of homes in Washington, DC in zones that allow for multi-family housing
- Proportion of homes in each quadrant of Washington, DC in zones that allow for multi-family housing

## 5. Questionnaire/Instrument

Although we did visit several of the sample units in person, not much relevant information could have been gained from going to our housing units. In particular, we did not have sufficient knowledge or information to accurately appraise a unit’s property value, nor could we have identified a unit’s zone district type. Nonetheless, even though we had to use online sources to obtain our data, we had to manually enter each address into a given website; in other words, the data for our parameters of interest were not already neatly organized, and it would not have been feasible for us to have obtained measurements for all the units in our sampling frame (thus necessitating a sample survey).      

Our primary instruments for measuring and capturing data were the government housing database web portals and Redfin:

- I. DC Zone Map 
  - http://maps.dcoz.dc.gov/zr16/
- II. Redfin 
  - https://www.redfin.com
- III. DC Tax Service Web Portal 
  - https://www.taxpayerservicecenter.com/RP_Search.jsp?search_type=Assessment

Zone district types were captured with the official DC Zoning Map (I). Once the street number, street name, and quadrant are entered, the website generates the address’s various zoning data, including its Zone district type. Once the type was identified, we recorded the following data points for each residential housing unit in our sample: (1) Street Number, (2) Street Name, (3) Quadrant, (4) Unit Number, (5) Unit Type, (6) Zone Type. 

Additionally, we captured the appraised values (Land and Improvement values) for each unit with Redfin (II) and the DC Tax Service web portal (III). First, with Redfin, we entered the street number, street name, quadrant, and city of each unit to return various property details, from which we obtained the Land and Addition (Improvement) dollar amounts of the unit. If we were not able to acquire the data points through Redfin, we then utilized the DC Tax Service web portal with the same data entry to obtain the Land and Improvement values of the properties. For condominiums and single-family units, we obtained unit-specific appraised values, while for multi-family apartments, we obtained their total building appraised values. These two additional data points were then appended to the above six data points for the selected sample units: (7) Land Value, (8) Improvement Value. 

## 6. Pilot Survey

In order to determine the sample allocation for our stratified random sample, we first needed to obtain an estimate for the variance of the appraised property values in our target population. By Theorem 9.1 (Wright 165), the sample variance $s^2$ is an unbiased estimator of the population variance $S^2$ under simple random sampling. Therefore, for our pilot survey, we took a simple random sample of size 25 from each of the four D.C. quadrants and combined these samples to obtain a stratified random sample of size 100 (though computers technically use deterministic methods, we used R to approximate the selection of this stratified random sample). 

By Problem 9.A.3 (Wright 169), when a variable of interest can take on only the values 0 and 1, the sample variance can be rewritten as $s^2 = \frac{n}{n - 1} \hat{p} (1 - \hat{p})$. Since $n$ is arbitrary, then this result also holds for $N$. In other words, when we are trying to estimate a proportion, the population variance can be written as $S^2 = \frac{N}{N - 1} p(1 - p)$. Since $p(1 - p)$ attains its maximum value at p = 0.5, we decided to be conservative by simply “assuming” that the proportion of housing units in our population that in zones that don’t restrict multi-family equals 0.5. Thus, we did not need to calculate the pilot sample variance for our zoning type parameter.

For the appraised property value, on the other hand, we attempted to enter each of the addresses into the DC Tax Service web portal and Redfin. Unfortunately, there were missing values for several of the housing units. In an attempt to rectify this problem, we recognized that since we took a simple random sample from each quadrant, then, within each quadrant, Theorem 9.2 (Wright 169) implies that the sample proportion of housing units with missing values is an unbiased estimator of the true proportion of housing units with missing values. Since it would not have been feasible to determine exactly how many housing units in our population have missing values, we therefore calculated the sample proportion of missing values for each quadrant and computed “adjusted” $N_h$ values for each of our four strata. Consequently, we ended up with an “adjusted” $N$ value, which was our estimated number of housing units that did not have missing values. These were the $N_h$ and $N$ values that we used in our sample selection and allocation algorithms.   

```{r write-pilot-survey}
set.seed(20190714)

pilot_sample <- sampling_frame %>%
  group_by(quadrant) %>%
  sample_n(25)

write_csv(pilot_sample, "data/pilot_sample.csv")

rm(pilot_sample)
```

```{r}
# load the completed pilot survey and clean the values
pilot_sample <- read_csv("data/pilot_sample_completed.csv") %>%
  mutate(land_value = ifelse(!is.na(rf_land_value), 
                             rf_land_value, 
                             land_value),
         improvement_value = ifelse(!is.na(rf_improvement_value), 
                                    rf_improvement_value, 
                                    improvement_value)) %>%
  mutate(property_value = land_value + improvement_value) %>%
  mutate(property_value = ifelse(unittype == "RENTAL" & 
                                   active_res_occupancy_count > 4 &
                                   property_value > 500000,
                                 property_value / active_res_occupancy_count,
                                 property_value
                                 ))
```

```{r pilot-summary-stats}
pilot_sample %>%
  summarize(mean = mean(property_value, na.rm = TRUE),
            s_squared_h = var(property_value, na.rm = TRUE), 
            missing_prop = mean(is.na(property_value))) %>%
  kable(caption = "Pilot survey summary statistics")  

pilot_sample %>%
  group_by(quadrant) %>%
  summarize(mean = mean(property_value, na.rm = TRUE),
            s_squared_h = var(property_value, na.rm = TRUE), 
            missing_prop = mean(is.na(property_value))) %>%
  kable(caption = "Pilot survey summary statistics by quadrant")
```

### Map of the pilot survey sample units

```{r pilot-map}
states %>%
  filter(state_name == "District of Columbia") %>%
  ggplot() +
  geom_polygon(aes(x = long, y = lat, group = group), 
               fill = "#d2d2d2",
               size = 0.3) +
  geom_point(data = pilot_sample,
             aes(x = longitude, y = latitude, color = quadrant),
             size = 2) +
  scale_color_manual(values = palette_urbn$categorical[[6]][c(1, 2, 5, 6)]) +
  coord_map() +
  labs(color = "Quadrant",
       x = NULL,
       y = NULL) +
  theme_urbn_map()
```

## 7. Determination of Sample and Strata Sizes

We were interested in estimating the sample mean of property values for all D.C. residences using stratification, the sample means of property values for all residences in each quadrant of D.C., the sample proportion of residences with different zoning types for all D.C. residences using stratifcation, and the sample proportions of residences with different zoning types in each quadrant of D.C.. All four of these four estimation processes required allocating a certain number of sample units to each strata and each process has a different optimal allocation. 

For the stratified estimates we specified a maximum variance, $V_0$, and used Exact Optimal Allocation for $\bar{y}_{str}$ and $\hat{p_{str}}$. For the estimates within each quadrant, we specified an error, $e$, and calculated an appropriate sample size for simple random sampling and a 90 percent confidence interval. Finally, we aligned all four optimal allocations and took the maximum $n_h$ for each quadrant. This didn't necessarily result in an optimal allocation for any one of our four estimates but it ensured that our allocation was adequate for each of our four estimates. 

### Condition 1: Sample mean

We began with a derivation of Exact Optimal Sample Allocation for $\bar{y}$.

Decomposition of $V(\bar{y}_h)$:

By Wright (12.4), $V(\bar{y}_{str}) = \sum_{h = 1}^H (\frac{N_h}{N})^2 V(\bar{y}_h) = \sum_{h = 1}^H (\frac{N_h}{N})^2 \frac{N_h - n_h}{N_h} \frac{S^2_h}{n_h}$

\hfill

$V(\bar{y}_h) = \frac{N_h - n_h}{N_h} \frac{S^2_h}{n_h}$ (By Wright Theorem 9.1, since we are taking a simple random sample from each stratum $h$)

$V(\bar{y}_h) = (1 - \frac{n_h}{N_h}) \frac{S^2_h}{n_h}$

$V(\bar{y}_h) = S_h^2(\frac{1}{n_h}) - \frac{S^2_h}{N_h}$

$V(\bar{y}_h) = S_h^2(1 - \frac{1}{1 \cdot 2} - \frac{1}{2 \cdot 3} - ...  - \frac{1}{n_h(n_h - 1)}) - \frac{S^2_h}{N_h}$

$V(\bar{y}_h) = \frac{(N_h - 1)S^2_h}{N_h} - \frac{S^2_h}{1 \cdot 2} - \frac{S^2_h}{2 \cdot 3} - ... - - \frac{S^2_h}{n_h(n_h - 1)}$

\hfill

Decomposition of $V(\bar{y}_{str})$

$V(\bar{y}_{str}) = \sum_{h = 1}^H V(\bar{y}_{str})$ (by the independence of $\bar{y}_1,...,\bar{y}$)

$V(\bar{y}_{str}) = \sum_{h = 1}^H \frac{N_h(N_h - 1)S^2_h}{N^2}$

$- \frac{N^2_1S^2_1}{N^2 \cdot 1 \cdot 2} - \frac{N^2_1S^2_1}{N^2 \cdot 2 \cdot 3} - ... - - \frac{N^2_1S^2_1}{N^2 n_1(n_1 - 1)}$

...

$- \frac{N^2_hS^2_h}{N^2 \cdot 1 \cdot 2} - \frac{N^2_hS^2_h}{N^2 \cdot 2 \cdot 3} - ... - - \frac{N^2_hS^2_h}{N^2 n_h(n_h - 1)}$

...

$- \frac{N^2_HS^2_H}{N^2 \cdot 1 \cdot 2} - \frac{N^2_HS^2_H}{N^2 \cdot 2 \cdot 3} - ... - - \frac{N^2_HS^2_H}{N^2 n_H(n_H - 1)}$

\hfill

For a desired bound $V_0$ on the sampling variance $V(\bar{y}_{str})$, one can find an optimal allocation using the following algorithm:

1) Assign, for each stratum, 1 unit to be selected for the sample. 

2) Fill in the following table and number these values starting from 1, in decreasing order. 

| | | | |
|---|---|---|---|
| $\frac{N^2_1S^2_1}{N^2 \cdot 1 \cdot 2}$ | $\frac{N^2_1S^2_1}{n^2 \cdot 2 \cdot 3}$ | $\frac{N^2_1S^2_1}{N^2 \cdot 3 \cdot 4}$ | ... |
| $\frac{N^2_2S^2_2}{N^2 \cdot 1 \cdot 2}$ | $\frac{N^2_2S^2_2}{N^2 \cdot 2 \cdot 3}$ | $\frac{N^2_2S^2_2}{N^2 \cdot 3 \cdot 4}$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\frac{N^2_HS^2_H}{N^2 \cdot 1 \cdot 2}$ | $\frac{N^2_HS^2_H}{N^2 \cdot 2 \cdot 3}$ | $\frac{N^2_HS^2_H}{N^2 \cdot 3 \cdot 4}$ | ... |

3) Since the initial allocation is $(n_{11}, n_{21}, ..., n_{H1}) = (1, 1, ..., 1)$, compute $V(\bar{y}_{str}|n_{11} = 1, n_{21} = 1, ..., n_{H1} = 1) = \sum_{h = 1}^H \frac{N_h(N_h - 1)S^2_h}{N^2}$

4) Pick value (1) from the table and increase the associated stratum's sample size by 1, o that the updated allocation is $(n_{12}, n_{22}, ..., n_{H2})$, where exactly one of the $n_{h2}$'s is equal to 2 and the rest are equal to 1. Then, compute $V(\bar{y}_{str}|n_{12}, ..., n_{H2} = V(\bar{y}_{str}|n_{11}, ..., n_{H1}) - \frac{1}{N^2}$ where "(1)" represents the largest value from the table. If $V(\bar{y}_{str}|N_{12}, ..., n_{H2} \leq V_0$, then stop with $n_1 = n_{12}, ..., N_H = N_{H2}$. Otherwise, go to step 5. 

5) Pick value (2) from the table and increase the associated stratum's sample size by 1, so that the updated allocation is $(n_{13}, ..., n_{H3})$. Then compute $V(\bar{y}_{str}|n_{13}, ..., n_{H3}) = V(\bar{y}_{str}|n_{12}, ..., n_{H2} - \frac{(2)}{N^2}$, where "(2)" represents the second value from the table. If $V(\bar{y}_{str}|n_{13}, ..., N_H = n_{H3}$. Otherwise, continue until step $j$, where $V(\bar{y}_str|n_{1j}, ..., n_{Hj}) \leq V_0$. The final allocation is $n_{1j}, ..., n_{Hj}$ and $n = n_{1j} + \cdot\cdot\cdot + n_{Hj}$.

\hfill

First we estimate $s^2$ and the missing proportion for each stratum.

```{r}
# find Nh and s2 for each strata
# (1) and (2)
s_squared_h <- pilot_sample %>%
  group_by(stratum = quadrant) %>%
  summarize(s_squared_h = var(property_value, na.rm = TRUE), 
            missing_prop = mean(is.na(property_value)))

Nh <- sampling_frame %>%
  count(stratum = quadrant) %>%
  rename(Nh = n)

strata <- left_join(s_squared_h, Nh, by = "stratum") %>%
  # adjust N because of missingness  
  mutate(Nh = Nh * (1 - missing_prop)) %>%
  mutate(N = sum(Nh))

rm(s_squared_h, Nh)

kable(strata,
      col.names = c("Stratum", "$s^2$", "Missing Prop.", "$N_h$", "N"))
```

Next we estimated the variance with an initial allocation of (1, 1, 1, 1) in each stratum. 

```{r mean-initial-allocation}
# Let the initial allocation be (n_11, n_21, n_31, n_41) = (1, 1, 1, 1) 
# (3) and (6)
starting_variance <- strata %>%
  mutate(strata_variance = Nh * (Nh - 1) * s_squared_h / N^2) 

kable(starting_variance,
      col.names = c("Stratum", "$s^2$", "Missing Prop.", "$N_h$", "N", "Initial variance"))

starting_variance <- starting_variance %>%
  summarize(V = sum(strata_variance)) %>%
  pull()

starting_variance
```

This resulted in a total starting variance of `r starting_variance`. Next we calculated priority values for each potential sample unit up to the entire sampling frame and ordered those priority values from largest to smallest. We then sequentially subtracted each additional prioirty value from the starting variance to get what we call a "marginal variance".

```{r mean-prioirty-values}
# create a table of priority values
# (4) and (5)
n_strata <-
  tibble(stratum = rep(strata$stratum, strata$Nh)) %>%
  group_by(stratum) %>%
  mutate(n = row_number()) %>%
  ungroup() %>%
  left_join(strata, by = "stratum")

# step 2
priority_values <- n_strata %>%
  group_by(stratum) %>%
  # rewritten to avoid integer overflow
  # mutate(priority_value = (Nh ^ 2 * s_squared_h) / (n * lag(n) * N ^ 2)) %>%
  mutate(priority_value = (Nh ^ 2 / n) * (s_squared_h / lag(n)) * (1 / N ^ 2)) %>%
  ungroup() %>%
  arrange(desc(priority_value))
```

```{r}
# (7)
priority_values <- priority_values %>%
  mutate(agg_priority_value = cumsum(priority_value)) %>%
  mutate(marginal_variance = starting_variance - agg_priority_value)

kable(head(select(priority_values, -missing_prop, -N), n = 10), digits = 0)

rm(n_strata)
```

We chose a $V_0$ of 0.1 times the mean from the pilot survey squared and then selected the largest priority values that achieved $V_0$ the quickest. 

```{r}
condition1 <- priority_values %>%
  mutate(stratum = factor(stratum)) %>%
  filter(marginal_variance >= ((0.1 * (mean(pilot_sample$property_value, na.rm = TRUE))) ^ 2))

condition1 <- condition1 %>%
  count(stratum, .drop = FALSE)
```

```{r}
priority_values %>%
  mutate(x = row_number()) %>%
  ggplot(aes(x = x, y = marginal_variance)) +
  geom_line() +
  scale_x_continuous(expand = c(0, 0),
                     limits = c(0, 350000)) + 
  scale_y_log10() +
  labs(title = "Variance of $\bar{y}_{str}$ with different Exact Optimal Allocations",
       x = "ith unit sampled",
       y = "V (log scale)")
```

### Condition 2: Sample means within strata

We were interested in comparing $\bar{y}_{h}$ from the four different quadrants. 

$n = \frac{N\sigma^2}{(N - 1) \frac{e^2}{z^2_{\frac{\alpha}{2}}} + \sigma^2}$

We used $s^2$ from our pilot survey as an unbiased estimate for $\sigma^2$. 

$n = \frac{Ns^2}{(N - 1) \frac{e^2}{z^2_{\frac{\alpha}{2}}} + s^2}$

We wanted $120,000 precision at a 90% confidence level for the mean of property value in each strata.  

```{r}
condition2 <- strata %>%
  mutate(n = (Nh * s_squared_h) / ((Nh - 1) * (120000 ^ 2 / qnorm(0.95) ^ 2) + s_squared_h)) 

condition2 %>%
  kable()
```

```{r include = FALSE}
rm(strata)
```

### Condition 3: Sample proportion

We began with a derivation of Exact Optimal Sample Allocation for $\hat{p}$.

Decomposition of $V(\hat{p}_{str})$

By Wright (12.14), $V(\hat{p}_{str}) = \sum_{h = 1}^H (\frac{N_h}{N})^2 V(p_h) = \sum_{h = 1}^H (\frac{N_h}{N})^2 \frac{N_h - n_h}{N_h - 1} \frac{p(1 - p)}{n_h}$

$V(\hat{p}_h) = (\frac{N_h}{N})^2 \frac{N_h - n_h}{N_h - 1} \frac{p(1 - p)}{n_h}$

$V(\hat{p}_h) = \frac{N^2_h}{N^2} \frac{N_h}{N_h - 1} \frac{p(1 - p)}{n_h} - \frac{N^2_h}{N^2} \frac{n_h}{N_h - 1}\frac{p(1 - p)}{n_h}$

$V(\hat{p}_h) = \frac{N^2_h}{N^2} \frac{N_h}{N_h - 1} \frac{p(1 - p)}{n_h} - \frac{N^2_h}{N^2} \frac{n_h}{N_h - 1}\frac{p(1 - p)}{n_h}$

$V(\hat{p}_h) = \frac{N^3_h p(1 - p)}{N^2 (N_h - 1)} \frac{1}{n_h} - \frac{N^2_h p(1 - p)}{N^2 (N_h - 1)}$

$V(\hat{p}_h) = \frac{N^3_h p(1 - p)}{N^2 (N_h - 1)} (1 - \frac{1}{1 \cdot 2} - \frac{1}{2 \cdot 3} - \cdot\cdot\cdot  - \frac{1}{n_h(n_h - 1)}) - \frac{N^2_h p(1 - p)}{N^2 (N_h - 1)}$

$V(\hat{p}_h) = \frac{N^3_h p(1 - p)}{N^2(N_h - 1)} - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 1 \cdot 2} - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 2 \cdot 3} - \cdot\cdot\cdot - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot n_h(n_h - 1)} - \frac{N^2_h p(1 - p)}{N^2 (N_h - 1)}$

$V(\hat{p}_h) = \frac{(N^3_h - N^2_h) p(1 - p)}{N^2(N_h - 1)} - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 1 \cdot 2} - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 2 \cdot 3} - \cdot\cdot\cdot - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot n_h(n_h - 1)}$

$V(\hat{p}_h) = \frac{N^2_h (N_h - 1) p(1 - p)}{N^2(N_h - 1)} - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 1 \cdot 2} - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 2 \cdot 3} - \cdot\cdot\cdot - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot n_h(n_h - 1)}$

\hfill

Decomposition of $V(\hat{p}_{str})$

$V(\hat{p}_{str}) = \sum_{h = 1}^H \frac{N^2_h (N_h - 1) p(1 - p)}{N^2(N_h - 1)}$


$- \frac{N^3_1 p(1 - p)}{N^2(N_1 - 1) \cdot 1 \cdot 2} - \frac{N^3_1 p(1 - p)}{N^2(N_1 - 1) \cdot 2 \cdot 3} - ... - \frac{N^3_1 p(1 - p)}{N^2(N_1 - 1) n_h(n_h - 1)}$

...

$- \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 1 \cdot 2} - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 2 \cdot 3} - ... - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) n_h (n_h - 1)}$

...

$- \frac{N^3_H p(1 - p)}{N^2(N_H - 1) \cdot 1 \cdot 2} - \frac{N^3_H p(1 - p)}{N^2(N_H - 1) \cdot 2 \cdot 3} - ... - \frac{N^3_H p(1 - p)}{N^2(N_H - 1) n_h (n_h - 1)}$

\hfill

For a desired bound on $V_0$ on the sampling variance $V(\hat{p}_{str})$, one may find an optimal allocation using the following algorithm:

1) Assign, for each stratum, 1 unit to be selected for the sample. 

2) Fill in the following table and number these values starting from 1, in decreasing order. We assume $p_h = 0.5$ because that is where the variance reaches its global maximum.

| | | | |
|---|---|---|---|
| $\frac{\frac{1}{4}N_1^3}{N^2 (N_1 - 1) \cdot 1 \cdot 2}$ | $\frac{\frac{1}{4}N_1^3}{N^2 (N_1 - 1) \cdot 2 \cdot 3}$ | $\frac{\frac{1}{4}N_1^3}{N^2 (N_1 - 1) \cdot 3 \cdot 4}$ | ... |
| $\frac{\frac{1}{4}N_2^3}{N^2 (N_2 - 1) \cdot 1 \cdot 2}$ | $\frac{\frac{1}{4}N_2^3}{N^2 (N_2 - 1) \cdot 2 \cdot 3}$ | $\frac{\frac{1}{4}N_2^3}{N^2 (N_2 - 1) \cdot 3 \cdot 4}$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\frac{\frac{1}{4}N_H^3}{N^2 (N_H - 1) \cdot 1 \cdot 2}$ | $\frac{\frac{1}{4}N_H^3}{N^2 (N_H - 1) \cdot 2 \cdot 3}$ | $\frac{\frac{1}{4}N_H^3}{N^2 (N_H - 1) \cdot 3 \cdot 4}$ | ... |

3) Since the initial allocation is $(n_{11}, n_{21}, ..., n_{H1}) = (1, 1, ..., 1)$, compute $V(\hat{p}_{str}|n_{11} = 1, n_{21} = 1, ..., n_{H1} = 1) = \frac{1}{N^2} \sum_{h = 1}^H ((N^2_h - N_h) S^2_h)$

4) Pick value (1) from the table and increase the associated stratum's sample size by 1, o that the updated allocation is $(n_{12}, n_{22}, ..., n_{H2})$, where exactly one of the $n_{h2}$'s is equal to 2 and the rest are equal to 1. Then, compute $V(\hat{p}_{str}|n_{12}, ..., n_{H2} = V(\hat{p}_{str}|n_{11}, ..., n_{H1}) - \frac{1}{N^2}$ where "(1)" represents the largest value from the table. If $V(\hat{p}_{str}|N_{12}, ..., n_{H2} \leq V_0$, then stop with $n_1 = n_{12}, ..., N_H = N_{H2}$. Otherwise, go to step 5. 

5) Pick value (2) from the table and increase the associated stratum's sample size by 1, so that the updated allocation is $(n_{13}, ..., n_{H3})$. Then compute $V(\hat{p}_{str}|n_{13}, ..., n_{H3}) = V(\hat{p}_{str}|n_{12}, ..., n_{H2} - \frac{(2)}{N^2}$, where "(2)" represents the second value from the table. If $V(\hat{p}_{str}|n_{13}, ..., N_H = n_{H3}$. Otherwise, continue until step $j$, where $V(\hat{p}_{str}|n_{1j}, ..., n_{Hj}) \leq V_0$. The final allocation is $n_{1j}, ..., n_{Hj})$ and $n = n_{1j} + \cdot\cdot\cdot + n_{Hj}$.

```{r}
# 
strata <- sampling_frame %>% 
  count(stratum = quadrant) %>% 
  rename(Nh = n) %>%
  mutate(N =sum(Nh),
         s_squared_h = 0.5 * (1 - 0.5))
  
kable(strata,
      col.names = c("Stratum", "$N_h$", "N", "$s^2_h"))
```

Next we estimated the variance with an initial allocation of (1, 1, 1, 1) in each stratum. 

```{r proportion-starting-allocation}
# Let the initial allocation be (n_11, n_21, n_31, n_41) = (1, 1, 1, 1) 
# (3) and (6)
starting_variance <- strata %>%
  mutate(strata_variance = (Nh ^ 2 * (Nh - 1) * 0.25) / (N ^ 2 * (Nh - 1))) 

kable(starting_variance,
      col.names = c("Stratum", "$N_h$", "N", "$s^2_h", "Initial variance"))

starting_variance <- starting_variance %>%
  summarize(V = sum(strata_variance)) %>%
  pull()
```

This resulted in a total starting variance of `r starting_variance`. Next we calculated priority values for each potential sample unit up to the entire sampling frame and ordered those priority values from largest to smallest. We then sequentially subtracted each additional prioirty value from the starting variance to get what we call a "marginal variance".

```{r}
# create a table of priority values
# (4) and (5)
n_strata <-
  sampling_frame %>%
  count(quadrant) 

n_strata <- tibble(stratum = rep(n_strata$quadrant, n_strata$n)) %>%
  group_by(stratum) %>%
  mutate(n = row_number()) %>%
  left_join(strata, by = "stratum")

# step 2
priority_values <- n_strata %>%
  group_by(stratum) %>%
  mutate(priority_value = (0.25 * Nh ^ 3) / (N ^ 2 * (Nh - 1) * n * lag(n))) %>%
  ungroup() %>%
  arrange(desc(priority_value))
```


```{r}
# (7)
priority_values <- priority_values %>%
  mutate(agg_priority_value = cumsum(priority_value)) %>%
  mutate(marginal_variance = starting_variance - agg_priority_value)

kable(head(select(priority_values, -N), n = 10), align = "l")

rm(n_strata)
```

We chose a $V_0$ of 0.1 times the mean from the pilot survey squared and then selected the largest priority values that achieved $V_0$ the quickest. 

```{r}
priority_values %>%
  mutate(x = row_number()) %>%
  ggplot(aes(x = x, y = marginal_variance)) +
  geom_line() +
  scale_x_continuous(expand = c(0, 0.05),
                     limits = c(0, 300000)) +
  scale_y_log10() +
  labs(x = "ith unit sampled",
       y = "V (log scale)")
```

```{r}
condition3 <- priority_values %>%
  filter(marginal_variance >= ((0.1 * 0.5) ^ 2))

condition3 <- count(condition3, stratum)
```

### Condition 4: Sample proportion within strata

We are interested in comparing $\hat{p}_h$ from the four different quadrants. 

$n = \frac{N p(1 - p)}{(N - 1) \frac{e^2}{z^2_{\frac{\alpha}{2}}} + p(1 - p)}$

We can assume that $p = 0.5$.  


$n = \frac{\frac{1}{4}N}{(N - 1) \frac{e^2}{z^2_{\frac{\alpha}{2}}} + \frac{1}{4}}$

We want 0.1 precision at a 90% confidence level for the mean of proportion with multi-family zoning in each strata.  

```{r}
condition4 <- strata %>%
  mutate(n = (Nh * 0.25) / ((Nh - 1) * (0.1 ^ 2 / qnorm(0.95) ^ 2) + 0.25)) 

condition4 %>%
  kable()
```

```{r include = FALSE}
rm(strata)
```

## Combining the above conditions

We want to sample at a rate that meets the four different requirements from above 

1. $V_0 > V(\bar{y}_{str})$ for the sample mean
2. \$50,000 precision at a 90% confidence level for $\bar{y}_h$ in each strata 
3. $V_0 > V(\hat{p}_h)$ for the sample proportion
4. 0.1 precision at a 90% confidence level for $\hat{p}$ in each strata

```{r}
tibble(quadrant = condition1$stratum,
       `1.` = condition1$n,
       `2.` = condition2$n,
       `3.` = condition3$n,
       `4.` = condition4$n) %>%
  kable(caption = "Recommended strata sizes across the four conditions")
```

```{r}
nh <- tibble(quadrant = condition1$stratum,
       `1.` = condition1$n,
       `2.` = condition2$n,
       `3.` = condition3$n,
       `4.` = condition4$n) %>%
  gather(key = "key", value = "nh", -quadrant) %>%
  group_by(quadrant) %>%
  summarize(nh = ceiling(max(nh))) 

nh %>%
  kable(caption = "Maximum recommended strata sizes across the four conditions")
```

## 8. Sampling Plan

```{r}
survey <- group_split(sampling_frame, quadrant) %>%
  map2_df(nh$nh, sample_n)

survey %>%
  sample_n(10) %>%
  select(status, fulladdress, res_type) %>%
  kable()
```

```{r}
write_csv(survey, "data/survey.csv")
```

### Map of the survey sample units

```{r survey-map}
states %>%
  filter(state_name == "District of Columbia") %>%
  ggplot() +
  geom_polygon(aes(x = long, y = lat, group = group), 
               fill = "#d2d2d2",
               size = 0.3) +
  geom_point(data = survey,
             aes(x = longitude, y = latitude, color = quadrant),
             size = 2) +
  scale_color_manual(values = palette_urbn$categorical[[6]][c(1, 2, 5, 6)]) +
  coord_map() +
  labs(color = "Quadrant",
       x = NULL,
       y = NULL) +
  theme_urbn_map()
```

## 9. Estimation

```{r}
Nh <- count(sampling_frame, quadrant)

represented_zones <- read_csv("data/represented-zones.csv")

final_survey <- read_csv("data/final-survey.csv") %>%
  mutate(property_value = land_value + improvement_value) %>%
  mutate(property_value = ifelse(unittype == "RENTAL" & 
                                   active_res_occupancy_count > 4 &
                                   property_value > 500000,
                                 property_value / active_res_occupancy_count,
                                 property_value
                                 ))

final_survey <- left_join(final_survey, Nh, by = "quadrant") %>%
  rename(fpc = n)

final_survey <- left_join(final_survey, represented_zones, by = "zoning")


strat_design <- svydesign(id = ~1, data = final_survey, strata = ~quadrant, fpc = ~fpc)
```

## Sample statistics





## $\mathbf{\bar{y}_{str}}$

The first parameter we estimated was the stratifed sample mean of the appraised property value in Washington DC. We also estimated a 95% confidence interval for the parameter. 

$\bar{y}_{str} = \sum_{h = 1}^H \frac{N_h}{N} \bar{y}_h$

\hfill

$\hat{V}(\bar{y}_h) = \sum_{h = 1}^H (\frac{N_h}{N})^2 \frac{N_h - n_h}{N_h} \frac{s^2_h}{n_h}$

\hfill

$(\bar{y}_{str} - Z_{\frac{\alpha}{2}} \sqrt{\hat{V}(\bar{y}_h)}, \bar{y}_{str} + Z_{\frac{\alpha}{2}} \sqrt{\hat{V}(\bar{y}_h)})$

\hfill

```{r stratified-mean}
tibble(
  Mean = svymean(~property_value, strat_design, na.rm = TRUE)[1],
  Lower = confint(svymean(~property_value, strat_design, na.rm = TRUE))[1],
  Upper = confint(svymean(~property_value, strat_design, na.rm = TRUE))[2]
) %>%
  kable(caption = "Mean appraised property value and 95% confidence interval",
        digits = 0)
```

## $\mathbf{\bar{y}_h}$

We were interested in comparing mean appraised property values across the District with each other. To this end, we treated each strata as its own simple random sample and calculated estimates and 95% confidence intervals for the parameters in each strata. 

$\bar{y}_h = \sum_{i = 1}^{n_h}$

\hfill

$s^2 = \frac{\sum_{i = 1}^{n_h} (y_i - \bar{y}_h)^2}{n_h - 1}$ where $i$ is the $ith$ observation in the strata. 

\hfill

$\hat{V}(\bar{y}_h) = (\frac{N_h - n_h}{N_h})^2 \frac{s^2}{n}$

```{r strata-means}
svyby(~property_value,            # variable to estimate
      ~quadrant,                  # subgroup variable
      design = strat_design,
      FUN = svymean,              # function to use on each subgroup
      keep.names = FALSE,         # does not include row.names 
      na.rm = TRUE,
      vartype = "ci"
      ) %>%
  kable(digits = 0)
```

## $\mathbf{\hat{p}_{str}}$

The third main parameter estimated was the stratifed proportion of addresses with multi-family zoning in Washington DC. We also estimated a 95% confidence interval for the parameter. 

$\hat{p}_{str} = \sum_{h = 1}^H \frac{N_h}{N} \hat{p}_h$

\hfill

$\hat{V}(\hat{p}_{str}) = \sum_{h = 1}^H (\frac{N_h}{N})^2 \frac{N_h - n_h}{N_h} \frac{\hat{p}_h(1 - \hat{p}_h}{n_h - 1}$

\hfill

$(\hat{p}_{str} - Z_{\frac{\alpha}{2}} \sqrt{\hat{V}(\hat{p}_{str})}, \hat{p}_{str} + Z_{\frac{\alpha}{2}} \sqrt{\hat{V}(\hat{p}_{str})})$

```{r}
tibble(
  Mean = svymean(~multifamily, strat_design)[1],
  Lower = confint(svymean(~multifamily, strat_design))[1],
  Upper = confint(svymean(~multifamily, strat_design))[2]
) %>%
  kable(caption = "",
        digits = 3)
```

## $\mathbf{\hat{p}_h}$

Finally, we were interested in comparing the proportion of addresses with multi-family zoning in quadrants across the District with each other. To this end, we treated each strata as its own simple random sample and calculated estimates and 95% confidence intervals for the parameters in each strata. 

$\hat{p}_h = \frac{\sum_{i = 1}^{n_h} y_i}{n_h}$ where $y_i$ is the indicator variable $$y_i = \begin{cases} 
      1 & \text{if the ith unit has the attribute} \\
      0 & \text{if the ith unit does not have the attribute}
   \end{cases}$$

\hfill
   
$\hat{V}(\hat{p}_h) = (\frac{N_h - n_h}{N_h}) \frac{\hat{p}_h(1 - \hat{p}_h)}{n_h - 1}$

\hfill

$(\hat{p}_h - Z_{\frac{\alpha}{2}} \sqrt{\hat{V}(\hat{p}_h)}, \hat{p}_h + Z_{\frac{\alpha}{2}} \sqrt{\hat{V}(\hat{p}_h)})$

```{r}
svyby(~multifamily,            # variable to estimate
      ~quadrant,                  # subgroup variable
      design = strat_design,
      FUN = svymean,              # function to use on each subgroup
      keep.names = FALSE,         # does not include row.names 
      na.rm = TRUE,
      vartype = "ci"
      ) %>%
  kable(digits = 3)
```

## 10. Discussion

### Survey conclusions

We estimated that homes are very valuable in Washington, DC. We estimated a home price of $478,338 with a 95% confidence interval of ($421,675, $535,001). That said, there is meaningful heterogeneity across quadrants with homes in Northwest DC being much more valuable than homes in the other three quadrants. 

We estimated fewer than half of homes in Washington, DC are in areas with zoning that does not restrict multi-family housing. We estimate that 0.417 of homes are in areas with zoning that does not restrict multi-family housing with a 95% confidence interval of (0.364, 0.471). This estimate is particularly dramatic because our sampling frame is based on addresses and not square area. In other words, the probability of selecting a home in an area with dense housing is greater than selecting a home in an area with sparse housing. It is likely that less than 0.417 of the residential land area in the District does not restrict multi-family housing. 

Like appraised property values, there is significant heterogeneity in the proportion of homes in different DC quadrants that are in zones that don’t restrict multi-family housing. The pattern, with NW being less restrictive than SE and NE, runs counter to the popular story that housing is most restricted in Northwest DC. This suggests that other limitations could be at play such as historical preservation, obstinate neighborhood associations, or restrictive permitting. The high share of homes in areas that allow for multi-family housing in SW reflects recent developments in Waterfront and The Wharf and also the high share of the quadrant that doesn’t have housing at all because of L’Enfant, Interstate 395, and the military base. 
 
### Process conclusions

> “You go to data analysis with the data you have, not the data you might want or wish to have at a later time.” ~ Anonymous

Data quality created challenges for estimations of mean appraised property values. Some units in the sampling frame, pilot survey, and final survey were missing values from both the DC Tax Service Web Portal and Redfin. We did our best to accommodate these missing values, but the uncertainty of our estimates almost certainly exceeded our calculated standard errors. 

We did not face the same challenges with zoning type, where measurement error was low to non-existent. We were able to fill in zoning type for all sampled units and classification into zones that allow for multi-family housing and zones that restrict multi-family housing was straightforward.  

Two key takeaways from this project:
1. Applications of probability sampling with low measurement error are more enjoyable than application with high measurement error or survey nonresponse. 
2. Measurement error and survey nonresponse are pressing issues in applications of probability sampling and warrant further exploration by the authors of this report. 

\newpage

## Appendix A

```{r all-code, ref.label=knitr::all_labels(), echo = TRUE, eval = FALSE, include = FALSE}
```

\newpage

## Bibliography