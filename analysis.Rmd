---
output:
  pdf_document
fontsize: 12pt
editor_options:
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r rmarkdown-setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
#knitr::opts_chunk$set(include = FALSE)
knitr::opts_chunk$set(size = "footnotesize")
knitr::opts_chunk$set(dev = "cairo_pdf")

options(scipen = 999)

def.chunk.hook <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size, "\n\n", x, "\n\n \\normalsize"), x)
})
```

# Zoning out: an analysis of zoning and property values in Washington, D.C.

### Brian Bontempo, Derrick Lee, and Aaron R. Williams

\newpage

## Introduction

## 1. Need Statement

Washington D.C. is consistently ranked as one of the most-expensive places to live in America (@Goetz19, @Frohlich19, @Kiersz19). A main driver of affordability challenges in the District is housing. Rents and home purchase prices have grown dramatically during the last two decades and affordability challenges have surged across the community's 68 square miles. At the same, Washington D.C.'s restrictive zoning, height limitation, and excessive historical preservation have constrained housing supply during a period of robust economic and population growth. 

This study aims to understand the cost of housing and the nature of zoning in Washington, D.C. and in its four quadrants. As subsequent sections will demonstrate, there are rich sources of information about property values and zoning rules in Washington, D.C., but it is difficult to extract information about population parameters from those sources. A probability sample of residences is to be taken to estimate the value of residential properties and the proportion of properties with zoning that allows for multi-family housing. 

## 2. Target Population
  - Clearly describe the *finite* population: All residential units in the District of Columbia. 

## 3. Sampling Frame

### Download the data

```{r r-setup}
library(tidyverse)
library(knitr)
library(urbnmapr)
library(urbnthemes)
```

```{r download-data}
# file path to csv with addresses
aru_file_path <- 
  "https://opendata.arcgis.com/datasets/c3c0ae91dca54c5d9ce56962fa0dd645_68.csv"

ap_file_path <- 
  "https://opendata.arcgis.com/datasets/aa514416aaf74fdc94748f1e56e7cc8a_0.csv"

# create a directory for downloading the data
if (!dir.exists("data/")) {
  dir.create("data")
}

# if the data doesn't already exist, download the data
if (!file.exists("data/aru.csv")) {
  download.file(aru_file_path, "data/aru.csv")
}

if (!file.exists("data/ap.csv")) {
  download.file(ap_file_path, "data/ap.csv")
}
```

### Address Residential Units

The first dataset is [Address Residential Units](http://opendata.dc.gov/datasets/address-residential-units)

The dataset does not contain a variable for quadrant, so we extract quadrant from the full address.

```{r load-aru}
aru <- read_csv("data/aru.csv") %>%
  rename_all(tolower) %>%
  select(unit_id, address_id, fulladdress, status, unitnum, unittype)

# extract quadrant
aru <- aru %>%
  mutate(quadrant = str_sub(fulladdress, start = -2, end = -1))
```

Address Residential Units contains residential units with status set to "RETIRED". We drop these cases as well. 

```{r}
count(aru, status) %>%
  kable()
```

```{r}
aru <- aru %>%
  filter(status != "RETIRE")
```

### Address Points

```{r load-ap}
# load the data and convert the variable names to lower case
ap <- read_csv("data/ap.csv", guess_max = 10000) %>%
  rename_all(tolower) %>%
  select(address_id, status, type_, entrancetype, quadrant, fulladdress, 
         objectid_1, assessment_nbhd, cfsa_name, census_tract, vote_prcnct, 
         ward, zipcode, anc, census_block, census_blockgroup, latitude, 
         longitude, active_res_unit_count, res_type, active_res_occupancy_count) 
```

Address Points contains residential units, non-residential units, and mixed-use units. Residential units and mixed-use units contain residences that belong to our sampling frame. We drop non-residential units. 

```{r}
count(ap, res_type) %>%
  kable()
```

```{r}
ap <- ap %>%
  filter(res_type != "NON RESIDENTIAL")
```

Address points contains residential units with status set to "RETIRED". We drop these cases as well. 

```{r}
count(ap, status) %>%
  kable()
```

```{r}
ap <- ap %>%
  filter(status != "RETIRE")
```

After the above filtering, there are 98 observations from Address Points and 3,706 observations in Address Residential Units that have missing addresses. We investigated joining the two datasets on `address_id` to fill in the address but all records missing an address in one dataset were missing an address in the other dataset. 

We dropped the missing values which represented about 1.5 percent of observations in Address Residential Units and 0.07 percent of observations in Address Points. 

```{r}
ap <- ap %>%
  filter(!is.na(fulladdress))

aru <- aru %>%
  filter(!is.na(fulladdress))
```

```{r include = FALSE}
missing_aru <- filter(aru, is.na(fulladdress))

missing_aru <- left_join(missing_aru, ap, by = "address_id")

# check the complement to see if any rows didn't match
if (nrow(anti_join(missing_aru, ap, by = "address_id")) > 0) "ERROR"


count(missing_aru, fulladdress.y)

missing_ap <- filter(ap, is.na(fulladdress))

missing_ap <- left_join(missing_ap, aru, by = "address_id")

if (nrow(anti_join(missing_ap, aru, by = "address_id")) > 0) "ERROR"

count(missing_ap, fulladdress.y)

rm(missing_ap, missing_aru)
```

### Merge variables

Address Points has interesting variables not present in Address Residential Units. So we merge the Address Points dataset with the Address Residential Units dataset. The join works for all but 572 cases, most of which are in a new building at the Wharf. 

```{r}
aru_expanded <- aru %>%
  select(-status) %>%
  left_join(ap, by = c("fulladdress", "address_id")) %>%
  select(quadrant = quadrant.x, everything(), -quadrant.y)

anti_join(aru, ap, by = c("fulladdress", "address_id"))

rm(aru)
```

### Combination

Next, we need to drop addresses in the Address Points dataset that exist in the Address Residential Units dataset so we don't over count addresses in multi-dwelling units. 

```{r}
ap <- ap %>%
  filter(!address_id %in% unique(aru_expanded$address_id))
```

Finally, we can combine the two datasets to create a sampling frame that contains approximately every residential address in Washington D.C.


```{r}
sampling_frame <- bind_rows(ap, aru_expanded) 

rm(ap, aru_expanded)

#summarize_all(addresses, list(~sum(is.na(.))))

write_csv(sampling_frame, "sampling_frame.csv")
```

## 4. Primary Parameters
  - D.C. sample mean of property values
  - Quadrant sample mean of property values
  - D.C. sample proportion of multi-family zoning
  - Quadrant sample proportion of multi-family zoning

## 5. Questionnaire/Instrument

## 6. Pilot Survey

```{r write-pilot-survey}
set.seed(20190714)

pilot_sample <- sampling_frame %>%
  group_by(quadrant) %>%
  sample_n(25)

write_csv(pilot_sample, "data/pilot_sample.csv")

rm(pilot_sample)
```

```{r}
# load the completed pilot survey and clean the values
pilot_sample <- read_csv("data/pilot_sample_completed.csv") %>%
  mutate(land_value = ifelse(!is.na(rf_land_value), 
                             rf_land_value, 
                             land_value),
         improvement_value = ifelse(!is.na(rf_improvement_value), 
                                    rf_improvement_value, 
                                    improvement_value)) %>%
  mutate(property_value = land_value + improvement_value) %>%
  mutate(property_value = ifelse(unittype == "RENTAL" & 
                                   active_res_occupancy_count > 4 &
                                   property_value > 500000,
                                 property_value / active_res_occupancy_count,
                                 property_value
                                 ))
```

```{r pilot-summary-stats}
pilot_sample %>%
  summarize(mean = mean(property_value, na.rm = TRUE),
            s_squared_h = var(property_value, na.rm = TRUE), 
            missing_prop = mean(is.na(property_value))) %>%
  kable(caption = "Pilot survey summary statistics")  

pilot_sample %>%
  group_by(quadrant) %>%
  summarize(mean = mean(property_value, na.rm = TRUE),
            s_squared_h = var(property_value, na.rm = TRUE), 
            missing_prop = mean(is.na(property_value))) %>%
  kable(caption = "Pilot survey summary statistics by quadrant")
```

### Map of the pilot survey sample units

```{r pilot-map}
states %>%
  filter(state_name == "District of Columbia") %>%
  ggplot() +
  geom_polygon(aes(x = long, y = lat, group = group), 
               fill = "#d2d2d2",
               size = 0.3) +
  geom_point(data = pilot_sample,
             aes(x = longitude, y = latitude, color = quadrant),
             size = 2) +
  scale_color_manual(values = palette_urbn$categorical[[6]][c(1, 2, 5, 6)]) +
  coord_map() +
  labs(color = "Quadrant",
       x = NULL,
       y = NULL) +
  theme_urbn_map()
```

## 7. Determination of Sample and Strata Sizes

### Condition 1: Sample mean

We begin with a derivation of Exact Optimal Sample Allocation for $\bar{y}$.

Decomposition of $V(\bar{y}_h)$:

By Wright (12.4), $V(\bar{y}_{str}) = \sum_{h = 1}^H (\frac{N_h}{N})^2 V(\bar{y}_h) = \sum_{h = 1}^H (\frac{N_h}{N})^2 \frac{N_h - n_h}{N_h} \frac{S^2_h}{n_h}$

\hfill

$V(\bar{y}_h) = (\frac{N_h}{N})^2 \frac{N_h - n_h}{N_h} \frac{S^2_h}{n_h}$

$V(\bar{y}_h) = (\frac{N_h^2}{N^2}) (1 - \frac{n_h}{N_h}) \frac{S^2_h}{n_h}$

$V(\bar{y}_h) = (\frac{N_h^2S_h^2}{N^2})(\frac{1}{n_h}) - \frac{N^2_h n_h S^2_h}{N^2 N_h n_h}$

$V(\bar{y}_h) = (\frac{N_h^2S_h^2}{N^2})(\frac{1}{n_h}) - \frac{N_h S^2_h}{N^2}$

$V(\bar{y}_h) = (\frac{N_h^2S_h^2}{N^2})(1 - \frac{1}{1 \cdot 2} - \frac{1}{2 \cdot 3} - ...  - \frac{1}{n_h(n_h - 1)}) - \frac{N_h S^2_h}{N^2}$

$V(\bar{y}_h) = \frac{N_h(N_h - 1)S^2_h}{N^2} - \frac{N^2_hS^2_h}{N^2 \cdot 1 \cdot 2} - \frac{N^2_hS^2_h}{N^2 \cdot 2 \cdot 3} - ... - - \frac{N^2_hS^2_h}{N^2 n_h(n_h - 1)}$

\hfill

Decomposition of $V(\bar{y}_{str})$

$V(\bar{y}_{str}) = \sum_{h = 1}^H \frac{N_h(N_h - 1)S^2_h}{N^2}$

$- \frac{N^2_1S^2_1}{N^2 \cdot 1 \cdot 2} - \frac{N^2_1S^2_1}{N^2 \cdot 2 \cdot 3} - ... - - \frac{N^2_1S^2_1}{N^2 n_1(n_1 - 1)}$

...

$- \frac{N^2_hS^2_h}{N^2 \cdot 1 \cdot 2} - \frac{N^2_hS^2_h}{N^2 \cdot 2 \cdot 3} - ... - - \frac{N^2_hS^2_h}{N^2 n_h(n_h - 1)}$

...

$- \frac{N^2_HS^2_H}{N^2 \cdot 1 \cdot 2} - \frac{N^2_HS^2_H}{N^2 \cdot 2 \cdot 3} - ... - - \frac{N^2_HS^2_H}{N^2 n_H(n_H - 1)}$

\hfill

For a desired bound $V_0$ on the sampling variance $V(\bar{y}_{str})$, we may find an optimal allocation using the following algorithm:

1) Assign, for each stratum, 1 unit to be selected for the sample. 

2) Fill in the following table and number these values starting from 1, in decreasing order. 

| | | | |
|---|---|---|---|
| $\frac{N^2_1S^2_1}{N^2 \cdot 1 \cdot 2}$ | $\frac{N^2_1S^2_1}{n^2 \cdot 2 \cdot 3}$ | $\frac{N^2_1S^2_1}{N^2 \cdot 3 \cdot 4}$ | ... |
| $\frac{N^2_2S^2_2}{N^2 \cdot 1 \cdot 2}$ | $\frac{N^2_2S^2_2}{N^2 \cdot 2 \cdot 3}$ | $\frac{N^2_2S^2_2}{N^2 \cdot 3 \cdot 4}$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\frac{N^2_HS^2_H}{N^2 \cdot 1 \cdot 2}$ | $\frac{N^2_HS^2_H}{N^2 \cdot 2 \cdot 3}$ | $\frac{N^2_HS^2_H}{N^2 \cdot 3 \cdot 4}$ | ... |

3) Since the initial allocation is $(n_{11}, n_{21}, ..., n_{H1}) = (1, 1, ..., 1)$, compute $V(\bar{y}_{str}|n_{11} = 1, n_{21} = 1, ..., n_{H1} = 1) = \sum_{h = 1}^H \frac{N_h(N_h - 1)S^2_h}{N^2}$

4) Pick value (1) from the table and increase the associated stratum's sample size by 1, o that the updated allocation is $(n_{12}, n_{22}, ..., n_{H2})$, where exactly one of the $n_{h2}$'s is equal to 2 and the rest are equal to 1. Then, compute $V(\bar{y}_{str}|n_{12}, ..., n_{H2} = V(\bar{y}_{str}|n_{11}, ..., n_{H1}) - \frac{1}{N^2}$ where "(1)" represents the largest value from the table. If $V(\bar{y}_{str}|N_{12}, ..., n_{H2} \leq V_0$, then stop with $n_1 = n_{12}, ..., N_H = N_{H2}$. Otherwise, go to step 5. 

5) Pick value (2) from the table and increase the associated stratum's sample size by 1, so that the updated allocation is $(n_{13}, ..., n_{H3})$. Then compute $V(\bar{y}_{str}|n_{13}, ..., n_{H3}) = V(\bar{y}_{str}|n_{12}, ..., n_{H2} - \frac{(2)}{N^2}$, where "(2)" represents the second value from the table. If $V(\bar{y}_{str}|n_{13}, ..., N_H = n_{H3}$. Otherwise, continue until step $j$, where $V(\bar{y}_str|n_{1j}, ..., n_{Hj}) \leq V_0$. The final allocation is $n_{1j}, ..., n_{Hj}$ and $n = n_{1j} + \cdot\cdot\cdot + n_{Hj}$.

\hfill

```{r}
# find Nh and s2 for each strata
# (1) and (2)
s_squared_h <- pilot_sample %>%
  group_by(stratum = quadrant) %>%
  summarize(s_squared_h = var(property_value, na.rm = TRUE), 
            missing_prop = mean(is.na(property_value)))

Nh <- sampling_frame %>%
  count(stratum = quadrant) %>%
  rename(Nh = n)

strata <- left_join(s_squared_h, Nh, by = "stratum") %>%
  # adjust N because of missingness  
  mutate(Nh = Nh * (1 - missing_prop)) %>%
  mutate(N = sum(Nh))

rm(s_squared_h, Nh)

kable(strata)
```

Step 3: $\hat{V}(\bar{y}|1, 1, 1, 1) = \sum_{h = 1}^H (\frac{N_h}{N})^2 \frac{N_h - n_h}{N_H} \frac{s^2_h}{n_h} = \sum_{h = 1}^H (\frac{N_h}{N})^2 \frac{N_h - 1}{N_H} \frac{s^2_h}{1}$ 

(Wright 12.5)

```{r mean-initial-allocation}
# Let the initial allocation be (n_11, n_21, n_31, n_41) = (1, 1, 1, 1) 
# (3) and (6)
starting_variance <- strata %>%
  mutate(strata_variance = Nh * (Nh - 1) * s_squared_h / N^2) 

kable(starting_variance)

starting_variance <- starting_variance %>%
  summarize(V = sum(strata_variance)) %>%
  pull()

starting_variance
```

Step 3: 

Priority value = $\frac{N_1^2 \cdot s_1^2}{N_1^2 \cdot n_h(n_h - 1)}$

```{r mean-prioirty-values}
# create a table of priority values
# (4) and (5)
n_strata <-
  tibble(stratum = rep(strata$stratum, strata$Nh)) %>%
  group_by(stratum) %>%
  mutate(n = row_number()) %>%
  ungroup() %>%
  left_join(strata, by = "stratum")

# step 2
priority_values <- n_strata %>%
  group_by(stratum) %>%
  # rewritten to avoid integer overflow
  # mutate(priority_value = (Nh ^ 2 * s_squared_h) / (n * lag(n) * N ^ 2)) %>%
  mutate(priority_value = (Nh ^ 2 / n) * (s_squared_h / lag(n)) * (1 / N ^ 2)) %>%
  ungroup() %>%
  arrange(desc(priority_value))

kable(head(select(priority_values, -missing_prop), n = 10))
```

Step 4:

```{r}
# (7)
priority_values <- priority_values %>%
  mutate(agg_priority_value = cumsum(priority_value)) %>%
  mutate(marginal_variance = starting_variance - agg_priority_value) %>%
  mutate(marginal_sd = sqrt(marginal_variance))

kable(head(select(priority_values, -missing_prop, -N), n = 100), digits = 0)

rm(n_strata)
```

```{r}
condition1 <- priority_values %>%
  mutate(stratum = factor(stratum)) %>%
  filter(marginal_variance >= ((0.1 * (mean(pilot_sample$property_value, na.rm = TRUE))) ^ 2))

condition1 <- condition1 %>%
  count(stratum, .drop = FALSE)
```

### Condition 2: Sample means within strata

We are interested in comparing $\bar{y}_{h}$ from the four different quadrants. 

$n = \frac{N\sigma^2}{(N - 1) \frac{e^2}{z^2_{\frac{\alpha}{2}}} + \sigma^2}$

We can use $s^2$ from our pilot survey as an unbiased estimate for $\sigma^2$. 

$n = \frac{Ns^2}{(N - 1) \frac{e^2}{z^2_{\frac{\alpha}{2}}} + s^2}$

We want $120,000 precision at a 90% confidence level for the mean of property value in each strata.  

```{r}
condition2 <- strata %>%
  mutate(n = (Nh * s_squared_h) / ((Nh - 1) * (120000 ^ 2 / qnorm(0.95) ^ 2) + s_squared_h)) 

condition2 %>%
  kable()
```

```{r include = FALSE}
rm(strata)
```

### Condition 3: Sample proportion

We begin with a derivation of Exact Optimal Sample Allocation for $\hat{p}$.

Decomposition of $V(\hat{p}_{str})$

By Wright (12.14), $V(\hat{p}_{str}) = \sum_{h = 1}^H (\frac{N_h}{N})^2 V(p_h) = \sum_{h = 1}^H (\frac{N_h}{N})^2 \frac{N_h - n_h}{N_h - 1} \frac{p(1 - p)}{n_h}$

$V(\hat{p}_h) = (\frac{N_h}{N})^2 \frac{N_h - n_h}{N_h - 1} \frac{p(1 - p)}{n_h}$

$V(\hat{p}_h) = \frac{N^2_h}{N^2} \frac{N_h}{N_h - 1} \frac{p(1 - p)}{n_h} - \frac{N^2_h}{N^2} \frac{n_h}{N_h - 1}\frac{p(1 - p)}{n_h}$

$V(\hat{p}_h) = \frac{N^2_h}{N^2} \frac{N_h}{N_h - 1} \frac{p(1 - p)}{n_h} - \frac{N^2_h}{N^2} \frac{n_h}{N_h - 1}\frac{p(1 - p)}{n_h}$

$V(\hat{p}_h) = \frac{N^3_h p(1 - p)}{N^2 (N_h - 1)} \frac{1}{n_h} - \frac{N^2_h p(1 - p)}{N^2 (N_h - 1)}$

$V(\hat{p}_h) = \frac{N^3_h p(1 - p)}{N^2 (N_h - 1)} (1 - \frac{1}{1 \cdot 2} - \frac{1}{2 \cdot 3} - \cdot\cdot\cdot  - \frac{1}{n_h(n_h - 1)}) - \frac{N^2_h p(1 - p)}{N^2 (N_h - 1)}$

$V(\hat{p}_h) = \frac{N^3_h p(1 - p)}{N^2(N_h - 1)} - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 1 \cdot 2} - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 2 \cdot 3} - \cdot\cdot\cdot - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot n_h(n_h - 1)} - \frac{N^2_h p(1 - p)}{N^2 (N_h - 1)}$

$V(\hat{p}_h) = \frac{(N^3_h - N^2_h) p(1 - p)}{N^2(N_h - 1)} - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 1 \cdot 2} - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 2 \cdot 3} - \cdot\cdot\cdot - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot n_h(n_h - 1)}$

$V(\hat{p}_h) = \frac{N^2_h (N_h - 1) p(1 - p)}{N^2(N_h - 1)} - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 1 \cdot 2} - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 2 \cdot 3} - \cdot\cdot\cdot - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot n_h(n_h - 1)}$

\hfill

Decomposition of $V(\hat{p}_{str})$

$V(\hat{p}_{str}) = \sum_{h = 1}^H \frac{N^2_h (N_h - 1) p(1 - p)}{N^2(N_h - 1)}$


$- \frac{N^3_1 p(1 - p)}{N^2(N_1 - 1) \cdot 1 \cdot 2} - \frac{N^3_1 p(1 - p)}{N^2(N_1 - 1) \cdot 2 \cdot 3} - ... - \frac{N^3_1 p(1 - p)}{N^2(N_1 - 1) n_h(n_h - 1)}$

...

$- \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 1 \cdot 2} - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) \cdot 2 \cdot 3} - ... - \frac{N^3_h p(1 - p)}{N^2(N_h - 1) n_h (n_h - 1)}$

...

$- \frac{N^3_H p(1 - p)}{N^2(N_H - 1) \cdot 1 \cdot 2} - \frac{N^3_H p(1 - p)}{N^2(N_H - 1) \cdot 2 \cdot 3} - ... - \frac{N^3_H p(1 - p)}{N^2(N_H - 1) n_h (n_h - 1)}$

\hfill

For a desired bound on $V_0$ on the sampling variance $V(\hat{p}_{str})$, we may find an optimal allocation using the following algorithm:

1) Assign, for each stratum, 1 unit to be selected for the sample. 

2) Fill in the following table and number these values starting from 1, in decreasing order. We assume $p_h = 0.5$ because that is where the variance reaches its global maximum.

| | | | |
|---|---|---|---|
| $\frac{\frac{1}{4}N_1^3}{N^2 (N_1 - 1) \cdot 1 \cdot 2}$ | $\frac{\frac{1}{4}N_1^3}{N^2 (N_1 - 1) \cdot 2 \cdot 3}$ | $\frac{\frac{1}{4}N_1^3}{N^2 (N_1 - 1) \cdot 3 \cdot 4}$ | ... |
| $\frac{\frac{1}{4}N_2^3}{N^2 (N_2 - 1) \cdot 1 \cdot 2}$ | $\frac{\frac{1}{4}N_2^3}{N^2 (N_2 - 1) \cdot 2 \cdot 3}$ | $\frac{\frac{1}{4}N_2^3}{N^2 (N_2 - 1) \cdot 3 \cdot 4}$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\cdot$ | $\cdot$ | $\cdot$ | ... |
| $\frac{\frac{1}{4}N_H^3}{N^2 (N_H - 1) \cdot 1 \cdot 2}$ | $\frac{\frac{1}{4}N_H^3}{N^2 (N_H - 1) \cdot 2 \cdot 3}$ | $\frac{\frac{1}{4}N_H^3}{N^2 (N_H - 1) \cdot 3 \cdot 4}$ | ... |

3) Since the initial allocation is $(n_{11}, n_{21}, ..., n_{H1}) = (1, 1, ..., 1)$, compute $V(\hat{p}_{str}|n_{11} = 1, n_{21} = 1, ..., n_{H1} = 1) = \frac{1}{N^2} \sum_{h = 1}^H ((N^2_h - N_h) S^2_h)$

4) Pick value (1) from the table and increase the associated stratum's sample size by 1, o that the updated allocation is $(n_{12}, n_{22}, ..., n_{H2})$, where exactly one of the $n_{h2}$'s is equal to 2 and the rest are equal to 1. Then, compute $V(\hat{p}_{str}|n_{12}, ..., n_{H2} = V(\hat{p}_{str}|n_{11}, ..., n_{H1}) - \frac{1}{N^2}$ where "(1)" represents the largest value from the table. If $V(\hat{p}_{str}|N_{12}, ..., n_{H2} \leq V_0$, then stop with $n_1 = n_{12}, ..., N_H = N_{H2}$. Otherwise, go to step 5. 

5) Pick value (2) from the table and increase the associated stratum's sample size by 1, so that the updated allocation is $(n_{13}, ..., n_{H3})$. Then compute $V(\hat{p}_{str}|n_{13}, ..., n_{H3}) = V(\hat{p}_{str}|n_{12}, ..., n_{H2} - \frac{(2)}{N^2}$, where "(2)" represents the second value from the table. If $V(\hat{p}_{str}|n_{13}, ..., N_H = n_{H3}$. Otherwise, continue until step $j$, where $V(\hat{p}_{str}|n_{1j}, ..., n_{Hj}) \leq V_0$. The final allocation is $n_{1j}, ..., n_{Hj})$ and $n = n_{1j} + \cdot\cdot\cdot + n_{Hj}$.

```{r}
# 
strata <- sampling_frame %>% 
  count(stratum = quadrant) %>% 
  rename(Nh = n) %>%
  mutate(N =sum(Nh),
         s_squared_h = 0.5 * (1 - 0.5))
  
kable(strata)
```

```{r proportion-starting-allocation}
# Let the initial allocation be (n_11, n_21, n_31, n_41) = (1, 1, 1, 1) 
# (3) and (6)
starting_variance <- strata %>%
  mutate(strata_variance = (Nh ^ 2 * (Nh - 1) * 0.25) / (N ^ 2 * (Nh - 1))) 

kable(starting_variance)

starting_variance <- starting_variance %>%
  summarize(V = sum(strata_variance)) %>%
  pull()

starting_variance
```

```{r}
# create a table of priority values
# (4) and (5)

n_strata <-
  sampling_frame %>%
  count(quadrant) 

n_strata <- tibble(stratum = rep(n_strata$quadrant, n_strata$n)) %>%
  group_by(stratum) %>%
  mutate(n = row_number()) %>%
  left_join(strata, by = "stratum")

# step 2
priority_values <- n_strata %>%
  group_by(stratum) %>%
  mutate(priority_value = (0.25 * Nh ^ 3) / (N ^ 2 * (Nh - 1) * n * lag(n))) %>%
  ungroup() %>%
  arrange(desc(priority_value))

kable(head(priority_values, n = 10))
```


```{r}
# (7)
priority_values <- priority_values %>%
  mutate(agg_priority_value = cumsum(priority_value)) %>%
  mutate(marginal_variance = starting_variance - agg_priority_value) %>%
  mutate(marginal_sd = sqrt(marginal_variance))

kable(head(select(priority_values, -N), n = 100), align = "l")

rm(n_strata)
```

```{r}
condition3 <- priority_values %>%
  filter(marginal_variance >= ((0.1 * 0.5) ^ 2))

condition3 <- count(condition3, stratum)
```

### Condition 4: Sample proportion within strata

We are interested in comparing $\hat{p}_{h}$ from the four different quadrants. 

$n = \frac{N p(1 - p)}{(N - 1) \frac{e^2}{z^2_{\frac{\alpha}{2}}} + p(1 - p)}$

We can assume that $p = 0.5$.  


$n = \frac{\frac{1}{4}N}{(N - 1) \frac{e^2}{z^2_{\frac{\alpha}{2}}} + \frac{1}{4}}$

We want 0.1 precision at a 90% confidence level for the mean of proportion with multi-family zoning in each strata.  

```{r}
condition4 <- strata %>%
  mutate(n = (Nh * 0.25) / ((Nh - 1) * (0.1 ^ 2 / qnorm(0.95) ^ 2) + 0.25)) 

condition4 %>%
  kable()
```

```{r include = FALSE}
rm(strata)
```

## Combining the above conditions

We want to sample at a rate that meets the four different requirements from above 

1. $V_0 > V(\bar{y}_{str})$ for the sample mean
2. \$50,000 precision at a 90% confidence level for $\bar{y}_h$ in each strata 
3. $V_0 > V(\hat{p}_h)$ for the sample proportion
4. 0.1 precision at a 90% confidence level for $\hat{p}$ in each strata

```{r}
tibble(quadrant = condition1$stratum,
       `1.` = condition1$n,
       `2.` = condition2$n,
       `3.` = condition3$n,
       `4.` = condition4$n) %>%
  kable(caption = "Recommended strata sizes across the four conditions")
```

```{r}
nh <- tibble(quadrant = condition1$stratum,
       `1.` = condition1$n,
       `2.` = condition2$n,
       `3.` = condition3$n,
       `4.` = condition4$n) %>%
  gather(key = "key", value = "nh", -quadrant) %>%
  group_by(quadrant) %>%
  summarize(nh = ceiling(max(nh))) 

nh %>%
  kable(caption = "Maximum recommended strata sizes across the four conditions")
```

## 8. Sampling Plan

```{r}
survey <- group_split(sampling_frame, quadrant) %>%
  map2_df(nh$nh, sample_n)

survey %>%
  sample_n(10) %>%
  select(status, fulladdress, res_type) %>%
  kable()

count(survey, quadrant)
```

```{r}
write_csv(survey, "data/survey.csv")
```

### Map of the survey sample units

```{r survey-map}
states %>%
  filter(state_name == "District of Columbia") %>%
  ggplot() +
  geom_polygon(aes(x = long, y = lat, group = group), 
               fill = "#d2d2d2",
               size = 0.3) +
  geom_point(data = survey,
             aes(x = longitude, y = latitude, color = quadrant),
             size = 2) +
  scale_color_manual(values = palette_urbn$categorical[[6]][c(1, 2, 5, 6)]) +
  coord_map() +
  labs(color = "Quadrant",
       x = NULL,
       y = NULL) +
  theme_urbn_map()
```

## 9. Estimation Plan

## 10. Other Analyses

\newpage

## Appendix A

```{r all-code, ref.label=knitr::all_labels(), echo = TRUE, eval = FALSE, include = FALSE}
```

\newpage

## Bibliography